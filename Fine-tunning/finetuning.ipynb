{"cells":[{"cell_type":"markdown","metadata":{"id":"HwggSsqUctAF"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tensorboard"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR: Could not find a version that satisfies the requirement tranformers (from versions: none)\n","ERROR: No matching distribution found for tranformers\n"]}],"source":["!pip install tranformers"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.0)\n","Requirement already satisfied: filelock in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (15.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.0)\n","Requirement already satisfied: requests>=2.32.2 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.24.6)\n","Requirement already satisfied: packaging in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.7)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: colorama in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\aguro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Tue_Feb_27_16:28:36_Pacific_Standard_Time_2024\n","Cuda compilation tools, release 12.4, V12.4.99\n","Build cuda_12.4.r12.4/compiler.33961263_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\aguro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import os\n","import pandas as pd\n","import transformers as tr\n","from datasets import load_dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":955,"status":"ok","timestamp":1725145389627,"user":{"displayName":"Agustin Rodriguez","userId":"12047289102767967167"},"user_tz":180},"id":"Js2ZvaQxcwc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n"]}],"source":["if torch.cuda.is_available():\n","    print(\"GPU is available\")\n","else:\n","    print(\"GPU is not available\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import tempfile\n","\n","tmpdir = tempfile.TemporaryDirectory()\n","local_training_root = tmpdir.name"]},{"cell_type":"markdown","metadata":{},"source":["## 1- Data Preparation\n","The first step of the fine-tuning process is to identify a specific task and supporting dataset. In this notebook we will consider the specific task to be classifying movies reviews. The idea is generally simple task where a movie review is provided as plain-text and we would like to determine whether or not the review was positive or negative.\n","\n","The [IMDB dataset](https://huggingface.co/datasets/imdb) can be leveraged as a supporting dataset for this task. The dataset conveniently provides both a training and a testing dataset with  labeled binary sentiments, as well as a dataset of unlabeled data."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["imdb_ds = load_dataset(\"imdb\")"]},{"cell_type":"markdown","metadata":{},"source":["# 2 - Select pre-trained model\n","The next step of the fine-tuning process is to select a pre-trained model. We will consider using the [T5](https://huggingface.co/docs/transformers/model_doc/t5) [[paper]](https://arxiv.org/pdf/1910.10683.pdf) family of models for our fine-tuning purposes. The T5 models are text-to-text transformers that have been trained on a multi-task mixture of unsupervised and supervised tasks. They are well suited for tasks such as summarizatin, translations, text classifications, question answering, and more.\n","The `t5-small` version of the T5 models has 60 million parameters. This slimmed down version will be sufficient for our purposes. "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model_checkpoint = \"t5-small\""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["current_directory = os.getcwd()\n","\n","cache_dir = os.path.join(current_directory, 'cache')"]},{"cell_type":"markdown","metadata":{},"source":["Hugging Face provides the [Auto*](https://huggingface.co/docs/transformers/model_doc/auto) suite of objects to conveniently instatiate the various componentes associated with a pre-trained model. Here, we use [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) to load in the tokenizer that is associated with the `t5-small` model."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# load the tokenizer that was used for the t5-small model\n","tokenizer = tr.AutoTokenizer.from_pretrained(\n","    model_checkpoint, cache_dir=cache_dir\n",") # Use a pre-cached model"]},{"cell_type":"markdown","metadata":{},"source":["As mentioned above, the IMDB dataset is a binary sentiment dataset. Its labels therefore are encoded as (-1 = unknown; 0 = negative; 1 = positive) values. In order to use this dataset with a text-to-text model like T5, the label set needs to be represented as a string. We will simply translate each label id to its corresponding string value.\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def to_tokens(tokenizer: tr.models.t5.tokenization_t5_fast.T5TokenizerFast, label_map: dict) -> callable:\n","    \"\"\"\n","    Given a `tokenizer` this closure will iterate through `x` and return the result of `apply()`.\n","    This function is mapped to a dataset and returned with ids and attention mask.\n","    \"\"\"\n","\n","    def apply(x) -> tr.tokenization_utils_base.BatchEncoding:\n","        \"\"\"From a formatted dataset `x` a batch encoding `token_res` is created.\"\"\"\n","        target_labels = [label_map[y] for y in x[\"label\"]]\n","        token_res = tokenizer(\n","            x[\"text\"],\n","            text_target=target_labels,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            padding=True,\n","        )\n","        return token_res\n","\n","    return apply\n","\n","imdb_label_lookup = {0: \"negative\", 1: \"positive\", -1: \"unknown\"}\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 25000/25000 [00:17<00:00, 1412.02 examples/s]\n"]}],"source":["imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n","tokenized_dataset = imdb_ds.map(\n","    imdb_to_tokens, batched=True, remove_columns=[\"text\", \"label\"]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## 3- Setup Training\n","\n","The model training process is highly configurable. The [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) class effectively exposes the configurable aspects of the process allowing one to customize them accordingly. Here, we will focus on setting up a training process that performs a single epoch of training with a batch size of 16. We will also leverage `adamw_torch` as the optimizer.\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["checkpoint_name = \"test-trainer\"\n","local_checkpoint_path = os.path.join(local_training_root, checkpoint_name)\n","training_args = tr.TrainingArguments(\n","    local_checkpoint_path,\n","    num_train_epochs=1,  # default number of epochs to train is 3\n","    per_device_train_batch_size=16,\n","    optim=\"adamw_torch\",\n","    report_to=[\"tensorboard\"],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["The pre-trained `t5-small` model can be loaded using the [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSeq2SeqLM) class.\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\aguro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aguro\\OneDrive\\Escritorio\\Neuro + IA\\cache\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n"]}],"source":["#load the pretrained model\n","model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n","    model_checkpoint, cache_dir=cache_dir\n",") # Use a pre-cached model"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# used to assist the trainer in batching the data\n","data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n","trainer = tr.Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator, #data_collator will ensure data availability, avoiding the delay of making sure that batches are ready for the GPU to process\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## 4- Train: Foundation model to Fine-tuned version of that model\n","Before starting the training process, let's turn on Tensorboard. This will allow us to monitor the training process as checkpoint logs are created.\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["tensorboard_display_dir = f\"{local_checkpoint_path}/runs\""]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"data":{"text/plain":["Reusing TensorBoard on port 6007 (pid 19732), started 0:21:15 ago. (Use '!kill 19732' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6007;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%load_ext tensorboard\n","%tensorboard --logdir '{tensorboard_display_dir}'"]},{"cell_type":"markdown","metadata":{},"source":["Start the fine-tuning process."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 500/1563 [03:01<06:03,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6184, 'grad_norm': 1.3956140279769897, 'learning_rate': 3.400511836212412e-05, 'epoch': 0.32}\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▍   | 1000/1563 [05:57<03:16,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.1405, 'grad_norm': 1.9732414484024048, 'learning_rate': 1.801023672424824e-05, 'epoch': 0.64}\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▌| 1500/1563 [08:56<00:22,  2.78it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.1304, 'grad_norm': 1.949489712715149, 'learning_rate': 2.015355086372361e-06, 'epoch': 0.96}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1563/1563 [09:25<00:00,  2.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 565.2882, 'train_samples_per_second': 44.225, 'train_steps_per_second': 2.765, 'train_loss': 0.28974149231718505, 'epoch': 1.0}\n"]}],"source":["trainer.train()\n","\n","# save model to the local checkpoint\n","trainer.save_model()\n","trainer.save_state()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# save fine-tuned model\n","final_model_path = os.path.join(current_directory, f'llm04_fine_tuning/{checkpoint_name}')\n","trainer.save_model(output_dir=final_model_path)"]},{"cell_type":"markdown","metadata":{},"source":["## 5- Predict"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\aguro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]}],"source":["reviews = [\"\"\"\n","'Despicable Me' is a cute and funny movie, but the plot is predictable and the characters are not very well-developed. Overall, it's a good movie for kids, but adults might find it a bit boring.\"\"\",\n","\"\"\" 'The Batman' is a dark and gritty take on the Caped Crusader, starring Robert Pattinson as Bruce Wayne. The film is a well-made crime thriller with strong performances and visuals, but it may be too slow-paced and violent for some viewers.\n","\"\"\",\n","\"\"\"\n","The Phantom Menace is a visually stunning film with some great action sequences, but the plot is slow-paced and the dialogue is often wooden. It is a mixed bag that will appeal to some fans of the Star Wars franchise, but may disappoint others.\n","\"\"\",\n","\"\"\"\n","I'm not sure if The Matrix and the two sequels were meant to have a tigh consistency but I don't think they quite fit together. They seem to have a reasonably solid arc but the features from the first aren't in the second and third as much, instead the second and third focus more on CGI battles and more visuals. I like them but for different reasons, so if I'm supposed to rate the trilogy I'm not sure what to say.\n","\"\"\",\n","]\n","inputs = tokenizer(reviews, return_tensors=\"pt\", truncation=True, padding=True)\n","pred = fine_tuned_model.generate(\n","    input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>classification</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\n'Despicable Me' is a cute and funny movie, b...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>'The Batman' is a dark and gritty take on the...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\\nThe Phantom Menace is a visually stunning fi...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\\nI'm not sure if The Matrix and the two seque...</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review classification\n","0  \\n'Despicable Me' is a cute and funny movie, b...       negative\n","1   'The Batman' is a dark and gritty take on the...       positive\n","2  \\nThe Phantom Menace is a visually stunning fi...       positive\n","3  \\nI'm not sure if The Matrix and the two seque...       negative"]},"metadata":{},"output_type":"display_data"}],"source":["pdf = pd.DataFrame(\n","    zip(reviews, tokenizer.batch_decode(pred, skip_special_tokens=True)),\n","    columns=[\"review\", \"classification\"],\n",")\n","display(pdf)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNmW01IdtgkK729Tvwyil5F","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
